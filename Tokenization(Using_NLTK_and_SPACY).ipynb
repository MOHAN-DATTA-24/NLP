{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFApuKUkJPThg4BgShHH0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOHAN-DATTA-24/NLP/blob/main/Tokenization(Using_NLTK_and_SPACY).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LFB4HGtNArOe"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"\n",
        "I want to get deeply into NLP. But I'm not sure! where to start, and I'm also unfamiliar with great resources.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RwGQ9gRA1FO",
        "outputId": "68a47a6b-0b5b-4231-f999-445d597c069c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I want to get deeply into NLP. But I'm not sure! where to start, and I'm also unfamiliar with great resources.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using NLTK**"
      ],
      "metadata": {
        "id": "T9550gmXp4My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-KFbqQ1CZYv",
        "outputId": "debe69cc-b329-4f24-a0ed-ceeda336d887"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoCd7mjXCqQ3",
        "outputId": "a5acdbfc-ff30-4997-c368-786398144d7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "##  Paragraphs ---> Sentence\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "vVEErR-YA1Bx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## full stop and exclamations are considered as line break or new sentence."
      ],
      "metadata": {
        "id": "xEcevNk-DViC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "eebRiH3OA02j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa9FpVxYA0zH",
        "outputId": "45635ed2-e916-4461-d005-6b0b0b7354ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfQOQYQ-A0rK",
        "outputId": "8c872254-64ec-4c91-c20f-7fe03eb960d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I want to get deeply into NLP.\n",
            "But I'm not sure!\n",
            "where to start, and I'm also unfamiliar with great resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## Paragraph ---> Words\n",
        "## Sentence ---> Words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "17ovt-C4A0np"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2gYMsAHA0li",
        "outputId": "972a65d8-a949-4873-e2fe-005e11715434"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'want',\n",
              " 'to',\n",
              " 'get',\n",
              " 'deeply',\n",
              " 'into',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'But',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'not',\n",
              " 'sure',\n",
              " '!',\n",
              " 'where',\n",
              " 'to',\n",
              " 'start',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'also',\n",
              " 'unfamiliar',\n",
              " 'with',\n",
              " 'great',\n",
              " 'resources',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_t_sJTSHO2X",
        "outputId": "7e746281-f3d8-47a6-e979-0c3008413fb5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxq6uWTdA0iu",
        "outputId": "e423dacf-ee67-4086-b360-fc9a8e2b6ce6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'want',\n",
              " 'to',\n",
              " 'get',\n",
              " 'deeply',\n",
              " 'into',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'But',\n",
              " 'I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'not',\n",
              " 'sure',\n",
              " '!',\n",
              " 'where',\n",
              " 'to',\n",
              " 'start',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'also',\n",
              " 'unfamiliar',\n",
              " 'with',\n",
              " 'great',\n",
              " 'resources',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(wordpunct_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9XoaRPNA0gk",
        "outputId": "abcf7346-abec-4031-f820-de4de33b418d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHn2kZJOHNJ4",
        "outputId": "8d05e79b-50c4-41ad-fe72-99b274486dc2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'want', 'to', 'get', 'deeply', 'into', 'NLP', '.']\n",
            "['But', 'I', \"'m\", 'not', 'sure', '!']\n",
            "['where', 'to', 'start', ',', 'and', 'I', \"'m\", 'also', 'unfamiliar', 'with', 'great', 'resources', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(wordpunct_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqmALpo8A0d-",
        "outputId": "9a600564-e37e-45fa-bda5-d4f7e8e191fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'want', 'to', 'get', 'deeply', 'into', 'NLP', '.']\n",
            "['But', 'I', \"'\", 'm', 'not', 'sure', '!']\n",
            "['where', 'to', 'start', ',', 'and', 'I', \"'\", 'm', 'also', 'unfamiliar', 'with', 'great', 'resources', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "QuAFJbgsA0bb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It considers full stop as a part of token in last word of the sentence.\n",
        "## EXCEPTION: But for the last full stop of a corpus it will be considered as a separate token."
      ],
      "metadata": {
        "id": "qtIVmHxGJCjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "xFCFDr4EA0Yz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifI4BP0wH50y",
        "outputId": "2fb94b90-2b0e-461c-d441-08ba84015203"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'want',\n",
              " 'to',\n",
              " 'get',\n",
              " 'deeply',\n",
              " 'into',\n",
              " 'NLP.',\n",
              " 'But',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'not',\n",
              " 'sure',\n",
              " '!',\n",
              " 'where',\n",
              " 'to',\n",
              " 'start',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'also',\n",
              " 'unfamiliar',\n",
              " 'with',\n",
              " 'great',\n",
              " 'resources',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using SPACY**"
      ],
      "metadata": {
        "id": "8nK-N4txp_i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SPACY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia6qaYkOpzgH",
        "outputId": "db4e1451-d99a-4fcc-8207-f8ec7498d102"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SPACY in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from SPACY) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from SPACY) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from SPACY) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from SPACY) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from SPACY) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from SPACY) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from SPACY) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from SPACY) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from SPACY) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from SPACY) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from SPACY) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from SPACY) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->SPACY) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->SPACY) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->SPACY) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SPACY) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SPACY) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SPACY) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->SPACY) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->SPACY) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->SPACY) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->SPACY) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->SPACY) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->SPACY) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "ezvsKifPpzcv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the paragraph into sentences\n",
        "sentences = list(nlp(corpus).sents)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlu2-7Xcpzak",
        "outputId": "27ebf1a5-cb4d-4b90-da7a-8765c96cc6c8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              " I want to get deeply into NLP.,\n",
              " But I'm not sure!,\n",
              " where to start, and I'm also unfamiliar with great resources.]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the paragraph into words\n",
        "words_paragraph = [token.text for token in nlp(corpus)]\n",
        "words_paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5cjzvOMpzX8",
        "outputId": "8dae38a3-7e08-4799-9c62-fbdc47350ae1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " 'I',\n",
              " 'want',\n",
              " 'to',\n",
              " 'get',\n",
              " 'deeply',\n",
              " 'into',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'But',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'not',\n",
              " 'sure',\n",
              " '!',\n",
              " 'where',\n",
              " 'to',\n",
              " 'start',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'also',\n",
              " 'unfamiliar',\n",
              " 'with',\n",
              " 'great',\n",
              " 'resources',\n",
              " '.',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words_paragraph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5huEZmyXrRXL",
        "outputId": "464e3882-82b9-4c96-fd62-cabd2b821acc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize each sentence into words\n",
        "words_sentences = [[token.text for token in sentence] for sentence in sentences]\n",
        "words_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWLSLm3pzVc",
        "outputId": "b37b6f93-4d12-4a93-c017-0228737a0be2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['\\n', 'I', 'want', 'to', 'get', 'deeply', 'into', 'NLP', '.'],\n",
              " ['But', 'I', \"'m\", 'not', 'sure', '!'],\n",
              " ['where',\n",
              "  'to',\n",
              "  'start',\n",
              "  ',',\n",
              "  'and',\n",
              "  'I',\n",
              "  \"'m\",\n",
              "  'also',\n",
              "  'unfamiliar',\n",
              "  'with',\n",
              "  'great',\n",
              "  'resources',\n",
              "  '.',\n",
              "  '\\n']]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6SBFMCppzTX",
        "outputId": "f390a85e-6b4e-4956-9241-9b9669eef600"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract noun chunks\n",
        "noun_chunks = list(nlp(corpus).noun_chunks)\n",
        "noun_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15niwz2NpzQ3",
        "outputId": "49cea94f-9cce-4238-de1a-27115f9d6da4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[I, NLP, I, I, great resources]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table border=\"1\" cellspacing=\"0\" cellpadding=\"5\">\n",
        "  <tr>\n",
        "    <th>Aspect</th>\n",
        "    <th>NLTK</th>\n",
        "    <th>Spacy</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Design Philosophy</td>\n",
        "    <td>Comprehensive, educational</td>\n",
        "    <td>Efficiency, production use</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Ease of Use</td>\n",
        "    <td>Flexible, beginner-friendly</td>\n",
        "    <td>User-friendly, intuitive</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Performance</td>\n",
        "    <td>May be slower</td>\n",
        "    <td>Optimized for speed</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Features</td>\n",
        "    <td>Wide range of tools</td>\n",
        "    <td>Core NLP tasks</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Language Support</td>\n",
        "    <td>Supports many languages</td>\n",
        "    <td>Robust support for English</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Community and Development</td>\n",
        "    <td>Large and active community</td>\n",
        "    <td>Growing community, actively maintained</td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "q5Xk7wtow1FK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**When to use NLTK:**\n",
        "\n",
        "### For educational purposes, such as teaching and learning NLP concepts.<br>\n",
        "### For research in natural language processing, where flexibility and experimentation are important.<br>\n",
        "### When you need a wide range of tools and resources for NLP tasks, including algorithms for tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and more."
      ],
      "metadata": {
        "id": "--ybsn5NxZNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**When to use Spacy:**\n",
        "\n",
        "### For building real-world NLP applications that require efficiency and performance.\n",
        "### When you need pre-trained models for common NLP tasks, such as part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "### When you want a streamlined API and easy integration into production systems, without sacrificing performance."
      ],
      "metadata": {
        "id": "b3F25CXDx1I0"
      }
    }
  ]
}